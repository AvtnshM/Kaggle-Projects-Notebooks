{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11758955,"sourceType":"datasetVersion","datasetId":7381935}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/avtnshm/clinical-modernbert-v-biomedicalmb-on-ddxplus-data?scriptVersionId=254542935\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"32301486-32fd-4793-a0af-d7ff8c485cec","_cell_guid":"224a04db-5bd8-4587-b729-a229fc7914ad","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:52:54.061485Z","iopub.execute_input":"2025-08-06T06:52:54.061699Z","iopub.status.idle":"2025-08-06T06:52:55.52769Z","shell.execute_reply.started":"2025-08-06T06:52:54.06168Z","shell.execute_reply":"2025-08-06T06:52:55.526999Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the train, test, and validate datasets\ntrain_df = pd.read_csv('/kaggle/input/mldataset/ddxplus/train.csv')\ntest_df = pd.read_csv('/kaggle/input/mldataset/ddxplus/test.csv')\nvalidate_df = pd.read_csv('/kaggle/input/mldataset/ddxplus/validate.csv')\n\n# Display the first few rows of the train dataset\ntrain_df.head()","metadata":{"_uuid":"4036b3bd-fad3-4d8d-8d3c-f4156e2f17e6","_cell_guid":"51ba50ab-7e1d-4411-80ea-4d76e9b39fbf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:52:55.528857Z","iopub.execute_input":"2025-08-06T06:52:55.529237Z","iopub.status.idle":"2025-08-06T06:53:15.10382Z","shell.execute_reply.started":"2025-08-06T06:52:55.529218Z","shell.execute_reply":"2025-08-06T06:53:15.103083Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport json\n\n# Load JSON files\nwith open('/kaggle/input/mldataset/ddxplus/release_conditions.json', 'r') as f:\n    conditions = json.load(f)\nwith open('/kaggle/input/mldataset/ddxplus/release_evidences.json', 'r') as f:\n    evidences = json.load(f)\n\n# Convert JSON to DataFrames for easier inspection (optional)\nconditions_df = pd.DataFrame.from_dict(conditions, orient='index')\nevidences_df = pd.DataFrame.from_dict(evidences, orient='index')\n\n# Display the first few rows\nprint(\"Conditions (Diseases):\")\nprint(conditions_df.head())\nprint(\"\\nEvidences (Symptoms):\")\nprint(evidences_df.head())","metadata":{"_uuid":"a84621e6-b5c8-488a-8610-455383baf770","_cell_guid":"c6a3a286-0705-4753-b562-7c3b7285cdff","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.104529Z","iopub.execute_input":"2025-08-06T06:53:15.104851Z","iopub.status.idle":"2025-08-06T06:53:15.165077Z","shell.execute_reply.started":"2025-08-06T06:53:15.10481Z","shell.execute_reply":"2025-08-06T06:53:15.164325Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_symptom_codes(symptom_dict, evidences_df):\n    return [evidences_df.loc[code][\"question_en\"] for code in symptom_dict.keys() if code in evidences_df.index]\n\nconditions_df[\"symptom_questions\"] = conditions_df[\"symptoms\"].apply(lambda x: map_symptom_codes(x, evidences_df))\nconditions_df[\"antecedent_questions\"] = conditions_df[\"antecedents\"].apply(lambda x: map_symptom_codes(x, evidences_df))","metadata":{"_uuid":"e4e841be-78b4-476d-aad5-8025db0473a5","_cell_guid":"6d8b980d-adde-4f1f-8a63-77fb8f5c21ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.166876Z","iopub.execute_input":"2025-08-06T06:53:15.167079Z","iopub.status.idle":"2025-08-06T06:53:15.20717Z","shell.execute_reply.started":"2025-08-06T06:53:15.167064Z","shell.execute_reply":"2025-08-06T06:53:15.206422Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(conditions_df[[\"condition_name\", \"symptom_questions\", \"antecedent_questions\"]].head())","metadata":{"_uuid":"29ddf820-fb5e-491f-81e4-353b2b36d558","_cell_guid":"a4c15f3e-f6a3-4668-8e01-c6baec6a28e3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.207933Z","iopub.execute_input":"2025-08-06T06:53:15.208163Z","iopub.status.idle":"2025-08-06T06:53:15.226933Z","shell.execute_reply.started":"2025-08-06T06:53:15.208147Z","shell.execute_reply":"2025-08-06T06:53:15.226244Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map PATHOLOGY to condition_name\ntrain_df['disease_name'] = train_df['PATHOLOGY'].map(lambda x: conditions.get(x, {}).get('condition_name', x))\n\n# Display the result\nprint(train_df[['PATHOLOGY', 'disease_name']].head())","metadata":{"_uuid":"f9b0c8a6-25fb-46ac-b730-b5f938666baa","_cell_guid":"bd5482d7-6b1f-4c0c-90dc-691e4f01b13b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.227569Z","iopub.execute_input":"2025-08-06T06:53:15.227765Z","iopub.status.idle":"2025-08-06T06:53:15.464588Z","shell.execute_reply.started":"2025-08-06T06:53:15.227751Z","shell.execute_reply":"2025-08-06T06:53:15.463881Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Delete the symptom_questions and antecedent_questions columns\nconditions_df = conditions_df.drop(columns=[\"symptom_questions\", \"antecedent_questions\"], errors=\"ignore\")\n\n# Verify the columns are removed\nprint(conditions_df.columns)","metadata":{"_uuid":"11bb14e3-47d8-4fe2-9dbd-555b058b02a3","_cell_guid":"1189726a-ad76-4c41-911a-e8bcc4f9d646","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.465274Z","iopub.execute_input":"2025-08-06T06:53:15.465461Z","iopub.status.idle":"2025-08-06T06:53:15.470566Z","shell.execute_reply.started":"2025-08-06T06:53:15.465446Z","shell.execute_reply":"2025-08-06T06:53:15.469996Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_symptom_codes(symptom_dict, evidences_df):\n    # Return list of English questions for valid symptom codes\n    return [evidences_df.loc[code, \"question_en\"] for code in symptom_dict.keys() if code in evidences_df.index]\n\n# Apply to symptoms and antecedents in conditions_df\nconditions_df[\"symptom_questions\"] = conditions_df[\"symptoms\"].apply(lambda x: map_symptom_codes(x, evidences_df))\nconditions_df[\"antecedent_questions\"] = conditions_df[\"antecedents\"].apply(lambda x: map_symptom_codes(x, evidences_df))\n\n# Display the result\nprint(conditions_df[[\"condition_name\", \"symptom_questions\", \"antecedent_questions\"]].head())","metadata":{"_uuid":"06955ea1-6a1e-455d-8213-1ea629993178","_cell_guid":"9477c1c1-2e2e-4463-8aae-9523d8935605","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.471358Z","iopub.execute_input":"2025-08-06T06:53:15.471665Z","iopub.status.idle":"2025-08-06T06:53:15.494119Z","shell.execute_reply.started":"2025-08-06T06:53:15.471636Z","shell.execute_reply":"2025-08-06T06:53:15.493464Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot top 10 diseases\nplt.figure(figsize=(10, 6))\ntrain_df['disease_name'].value_counts().head(10).plot(kind='bar', color='#1f77b4')\nplt.title('Top 10 Most Frequent Diseases')\nplt.xlabel('Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"_uuid":"d962a62a-36fc-4e4b-a896-aad00f61fdde","_cell_guid":"4990ac02-63f1-48ba-bc56-5c91bc2644df","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:15.49474Z","iopub.execute_input":"2025-08-06T06:53:15.49492Z","iopub.status.idle":"2025-08-06T06:53:16.571977Z","shell.execute_reply.started":"2025-08-06T06:53:15.494906Z","shell.execute_reply":"2025-08-06T06:53:16.571262Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Column names from conditions_df\nprint(\"Conditions DataFrame columns:\")\nprint(conditions_df.columns.tolist())\n\n# Column names from evidences_df\nprint(\"\\nEvidences DataFrame columns:\")\nprint(evidences_df.columns.tolist())\n\n\nprint(\"Train columns:\", train_df.columns.tolist())\nprint(\"Validate columns:\", validate_df.columns.tolist())\nprint(\"Test columns:\", test_df.columns.tolist())","metadata":{"_uuid":"6ca75f20-bbc7-41af-b8f2-a58b062cd8f3","_cell_guid":"a644992e-a57e-46bf-b107-8865302ce501","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:16.574257Z","iopub.execute_input":"2025-08-06T06:53:16.574536Z","iopub.status.idle":"2025-08-06T06:53:16.579182Z","shell.execute_reply.started":"2025-08-06T06:53:16.57452Z","shell.execute_reply":"2025-08-06T06:53:16.578645Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers -q\nfrom transformers import AutoTokenizer, AutoModel\nimport torch","metadata":{"_uuid":"90ce3d62-747e-4e60-9bea-af4bb7a6c501","_cell_guid":"8f570896-a094-4487-b580-bc27a3fbe6a5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T06:53:16.579784Z","iopub.execute_input":"2025-08-06T06:53:16.579958Z","iopub.status.idle":"2025-08-06T06:53:27.767091Z","shell.execute_reply.started":"2025-08-06T06:53:16.579936Z","shell.execute_reply":"2025-08-06T06:53:27.766222Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CMB = AutoModel.from_pretrained('Simonlee711/Clinical_ModernBERT')\ntokenizer1 = AutoTokenizer.from_pretrained('Simonlee711/Clinical_ModernBERT')\nCMB.__class__","metadata":{"_uuid":"1b5845f4-a2f7-4e96-afce-d9a70e3b1406","_cell_guid":"37c2f2f1-1e06-4ed3-aa42-a06b0309364d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T07:48:51.718033Z","iopub.execute_input":"2025-08-06T07:48:51.718506Z","iopub.status.idle":"2025-08-06T07:48:52.671251Z","shell.execute_reply.started":"2025-08-06T07:48:51.718485Z","shell.execute_reply":"2025-08-06T07:48:52.670496Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel\n\nBioClinical_ModernBERT = \"thomas-sounack/BioClinical-ModernBERT-base\"\n\ntokenizer2 = AutoTokenizer.from_pretrained(BioClinical_ModernBERT)\nBMB = AutoModel.from_pretrained(BioClinical_ModernBERT)\n\nBMB.__class__","metadata":{"_uuid":"a52f80dc-c327-466d-9fc8-6502a566a920","_cell_guid":"9b0212fb-5f31-4d73-983e-7c8cf11fee7d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T07:49:01.985993Z","iopub.execute_input":"2025-08-06T07:49:01.986536Z","iopub.status.idle":"2025-08-06T07:49:02.768506Z","shell.execute_reply.started":"2025-08-06T07:49:01.986514Z","shell.execute_reply":"2025-08-06T07:49:02.76792Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df['DIFFERENTIAL_DIAGNOSIS'].nunique())\nprint(test_df['DIFFERENTIAL_DIAGNOSIS'].nunique())\nvalidate_df['DIFFERENTIAL_DIAGNOSIS'].nunique()","metadata":{"_uuid":"a8ecc4fa-36b4-4023-bafe-14471a33ea92","_cell_guid":"a653203b-97ba-4d8a-baba-b2993a12bd38","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-06T07:02:15.288205Z","iopub.execute_input":"2025-08-06T07:02:15.288483Z","iopub.status.idle":"2025-08-06T07:02:16.308634Z","shell.execute_reply.started":"2025-08-06T07:02:15.288461Z","shell.execute_reply":"2025-08-06T07:02:16.307903Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Helper: get English symptom questions from evidence codes\ndef get_symptom_text(evidence_codes, evidences_df):\n    return \" \".join([\n        evidences_df.loc[code][\"question_en\"]\n        for code in evidence_codes.split(';') if code in evidences_df.index\n    ])\n\n# 1. Create text column for embedding input\ntrain_df['symptom_text'] = train_df['EVIDENCES'].apply(lambda x: get_symptom_text(x, evidences_df))\n\n# 2. Function to get CLS embedding from a model\ndef get_cls_embedding(text_list, tokenizer, model):\n    model.eval()\n    embeddings = []\n\n    for text in text_list:\n        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n        embeddings.append(cls_embedding.squeeze().numpy())\n    \n    return np.array(embeddings)\n\n# 3. Generate embeddings for both models\nprint(\"Generating embeddings from Clinical_ModernBERT (CMB)...\")\ncmb_embeddings = get_cls_embedding(train_df['symptom_text'], tokenizer1, CMB)\n\nprint(\"Generating embeddings from BioClinical_ModernBERT (BMB)...\")\nbmb_embeddings = get_cls_embedding(train_df['symptom_text'], tokenizer2, BMB)\n\n# 4. Save embeddings if needed (optional)\nnp.save(\"/kaggle/working/cmb_embeddings.npy\", cmb_embeddings)\nnp.save(\"/kaggle/working/bmb_embeddings.npy\", bmb_embeddings)\n\nprint(\"Embedding shapes — CMB:\", cmb_embeddings.shape, \"| BMB:\", bmb_embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T08:04:27.16614Z","iopub.execute_input":"2025-08-06T08:04:27.166817Z","iopub.status.idle":"2025-08-06T09:12:48.855716Z","shell.execute_reply.started":"2025-08-06T08:04:27.166795Z","shell.execute_reply":"2025-08-06T09:12:48.854133Z"}},"outputs":[{"name":"stdout","text":"Generating embeddings from Clinical_ModernBERT (CMB)...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2009024276.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 3. Generate embeddings for both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating embeddings from Clinical_ModernBERT (CMB)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcmb_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cls_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'symptom_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCMB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating embeddings from BioClinical_ModernBERT (BMB)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2009024276.py\u001b[0m in \u001b[0;36mget_cls_embedding\u001b[0;34m(text_list, tokenizer, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mcls_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 )\n\u001b[1;32m    661\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    663\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 482\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":49},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:15:49.062676Z","iopub.execute_input":"2025-08-06T09:15:49.062963Z","iopub.status.idle":"2025-08-06T09:15:49.403553Z","shell.execute_reply.started":"2025-08-06T09:15:49.062941Z","shell.execute_reply":"2025-08-06T09:15:49.402846Z"}},"outputs":[{"name":"stdout","text":"Wed Aug  6 09:15:49 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# Helper: get English symptom questions from evidence codes\ndef get_symptom_text(evidence_codes, evidences_df):\n    return \" \".join([\n        evidences_df.loc[code][\"question_en\"]\n        for code in evidence_codes.split(';') if code in evidences_df.index\n    ])\n\n# Take a small sample of 100 rows\nsample_df = train_df.sample(n=100, random_state=42).copy()\n\n# Create text column for embedding input\nsample_df['symptom_text'] = sample_df['EVIDENCES'].apply(lambda x: get_symptom_text(x, evidences_df))\n\n# Function to get CLS embedding from a model\ndef get_cls_embedding(text_list, tokenizer, model):\n    model.eval()\n    embeddings = []\n\n    for text in text_list:\n        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n        embeddings.append(cls_embedding.squeeze().numpy())\n\n    return np.array(embeddings)\n\n# Generate embeddings for both models\nprint(\"Generating embeddings from Clinical_ModernBERT (CMB)...\")\ncmb_embeddings = get_cls_embedding(sample_df['symptom_text'], tokenizer1, CMB)\n\nprint(\"Generating embeddings from BioClinical_ModernBERT (BMB)...\")\nbmb_embeddings = get_cls_embedding(sample_df['symptom_text'], tokenizer2, BMB)\n\n# Save embeddings (optional)\nnp.save(\"/kaggle/working/cmb_embeddings_sample.npy\", cmb_embeddings)\nnp.save(\"/kaggle/working/bmb_embeddings_sample.npy\", bmb_embeddings)\n\nprint(\"Embedding shapes — CMB:\", cmb_embeddings.shape, \"| BMB:\", bmb_embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:15:11.399162Z","iopub.execute_input":"2025-08-06T09:15:11.399471Z","iopub.status.idle":"2025-08-06T09:15:19.414217Z","shell.execute_reply.started":"2025-08-06T09:15:11.39944Z","shell.execute_reply":"2025-08-06T09:15:19.413502Z"}},"outputs":[{"name":"stdout","text":"Generating embeddings from Clinical_ModernBERT (CMB)...\nGenerating embeddings from BioClinical_ModernBERT (BMB)...\nEmbedding shapes — CMB: (100, 768) | BMB: (100, 768)\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}